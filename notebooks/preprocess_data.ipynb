{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "from pytubefix import YouTube\n",
    "\n",
    "import collections\n",
    "\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "import nltk  \n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### engine for pose estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_options = python.BaseOptions(model_asset_path='../models/pose_landmarker.task')\n",
    "# options = vision.PoseLandmarkerOptions(\n",
    "#     base_options=base_options,\n",
    "#     output_segmentation_masks=True)\n",
    "# mp_pose = mp.solutions.pose\n",
    "# detector = mp_pose.Pose(static_image_mode=False,min_detection_confidence=0.8,min_tracking_confidence=0.8)\n",
    "# #detector = vision.PoseLandmarker.create_from_options(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mediapipe.python.solutions import drawing_utils, pose\n",
    "\n",
    "\n",
    "# def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "#     pose_landmarks_list = detection_result.pose_landmarks.landmark\n",
    "#     annotated_image = np.copy(rgb_image)\n",
    "\n",
    "#     # Create NormalizedLandmarkList to hold the detected landmarks\n",
    "#     pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "#     pose_landmarks_proto.landmark.extend([\n",
    "#         landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z)\n",
    "#         for landmark in pose_landmarks_list\n",
    "#     ])\n",
    "\n",
    "#     # Draw landmarks on the image\n",
    "#     drawing_utils.draw_landmarks(\n",
    "#         annotated_image,\n",
    "#         pose_landmarks_proto,\n",
    "#         pose.POSE_CONNECTIONS,  # Ensure POSE_CONNECTIONS matches the number of landmarks detected\n",
    "#         drawing_utils.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2),\n",
    "#         drawing_utils.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2)\n",
    "#     )\n",
    "\n",
    "#     return annotated_image\n",
    "\n",
    "\n",
    "# def calculate_angle(p1, p2):\n",
    "#     x_diff = p2[0] - p1[0]\n",
    "#     y_diff = p2[1] - p1[1]\n",
    "#     return np.degrees(np.arctan2(y_diff, x_diff))\n",
    "\n",
    "# # Funkcja do obliczania kąta obrotu sylwetki\n",
    "# def calculate_body_rotation_angle(landmarks):\n",
    "#     if landmarks is None:\n",
    "#       return None\n",
    "#     # Wykryte punkty charakterystyczne dla ramion (np. 11 i 12 dla lewego i prawego ramienia)\n",
    "#     left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER].x,\n",
    "#                      landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER].y]\n",
    "#     right_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER].x,\n",
    "#                       landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER].y]\n",
    "\n",
    "#     # Oblicz kąt nachylenia linii ramion\n",
    "#     angle = calculate_angle(left_shoulder, right_shoulder)\n",
    "#     return angle\n",
    "\n",
    "# def calculate_angle_3d(p1, p2):\n",
    "#     x_diff = p2[0] - p1[0]\n",
    "#     y_diff = p2[1] - p1[1]\n",
    "#     z_diff = p2[2] - p1[2]\n",
    "#     # Obliczamy kąt w płaszczyźnie xy (poziomej)\n",
    "#     angle_xy = np.degrees(np.arctan2(y_diff, x_diff))\n",
    "#     # Obliczamy kąt w płaszczyźnie yz (pionowej)\n",
    "#     angle_yz = np.degrees(np.arctan2(z_diff, y_diff))\n",
    "#     return angle_xy, angle_yz\n",
    "\n",
    "# # Funkcja do obliczania kąta obrotu sylwetki w 3D\n",
    "# def calculate_body_rotation_angle_3d(landmarks_3d):\n",
    "#     if landmarks_3d is None:\n",
    "#         return None\n",
    "#     # Wykryte punkty charakterystyczne dla ramion w 3D\n",
    "#     left_shoulder_3d = [landmarks_3d['left_shoulder_x'],\n",
    "#                         landmarks_3d['left_shoulder_y'],\n",
    "#                         landmarks_3d['left_shoulder_z']]\n",
    "#     right_shoulder_3d = [landmarks_3d['right_shoulder_x'],\n",
    "#                          landmarks_3d['right_shoulder_y'],\n",
    "#                          landmarks_3d['right_shoulder_z']]\n",
    "\n",
    "#     # Oblicz kąt nachylenia linii ramion w płaszczyźnie xy i yz\n",
    "#     angle_xy, angle_yz = calculate_angle_3d(left_shoulder_3d, right_shoulder_3d)\n",
    "#     return angle_xy, angle_yz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#xwyPjhRoeNc\n",
    "#nhoikoUEI8U\n",
    "video_id = \"nhoikoUEI8U\"\n",
    "subtitles = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "print(len(subtitles))\n",
    "\n",
    "yt = YouTube(f\"https://www.youtube.com/watch?v={video_id}\")\n",
    "stream = yt.streams.get_highest_resolution()\n",
    "\n",
    "#print(yt.streams.filter(res=\"720p\").first())\n",
    "destination_path = \"../videos\" \n",
    "\n",
    "video_file = stream.download(output_path=destination_path)\n",
    "\n",
    "\n",
    "# def cv2_to_mediapipe_image(cv2_image):\n",
    "#     rgb_image = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB)\n",
    "#     image = mp.solutions.mediapipe.python.solution_base.Image(\n",
    "#         width=rgb_image.shape[1],\n",
    "#         height=rgb_image.shape[0],\n",
    "#         rgb_data=np.frombuffer(rgb_image.tobytes(), dtype=np.uint8)\n",
    "#     )\n",
    "\n",
    "#     return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Funkcja do skalowania obrazu\n",
    "# def resize_image(image, target_size):\n",
    "#     height, width = image.shape[:2]\n",
    "#     if height > width:\n",
    "#         scale = target_size / height\n",
    "#     else:\n",
    "#         scale = target_size / width\n",
    "#     new_height = int(height * scale)\n",
    "#     new_width = int(width * scale)\n",
    "#     resized_image = cv2.resize(image, (new_width, new_height))\n",
    "#     padded_image = np.zeros((target_size, target_size, 3), dtype=np.uint8)\n",
    "#     padded_image[:new_height, :new_width, :] = resized_image\n",
    "#     return padded_image\n",
    "\n",
    "# # Wczytaj model MoveNet\n",
    "# model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
    "# movenet = model.signatures['serving_default']\n",
    "\n",
    "# # Wczytaj wideo\n",
    "# cap = cv2.VideoCapture(video_file)\n",
    "# current_frame = 0\n",
    "# last_frame = 0\n",
    "# while cap.isOpened():\n",
    "#     cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "#     if current_frame !=last_frame:\n",
    "\n",
    "#         # Skaluj obraz do rozmiaru akceptowalnego przez MoveNet\n",
    "#         input_image = resize_image(frame, 192)\n",
    "\n",
    "#         # Przygotuj obraz do przetwarzania przez model\n",
    "#         input_image = tf.image.convert_image_dtype(input_image, dtype=tf.float32)\n",
    "#         input_image = tf.expand_dims(input_image, axis=0)\n",
    "#         input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "\n",
    "#         # Wykrywanie postury\n",
    "#         keypoints_with_scores = movenet(input_image)['output_0'].numpy()\n",
    "\n",
    "#         # Przetwarzanie wyników\n",
    "#         keypoints = keypoints_with_scores[0, 0, :, :2]\n",
    "#         scores = keypoints_with_scores[0, 0, :, 2]\n",
    "#         print(scores)\n",
    "#         print(keypoints)\n",
    "#         # Rysowanie wykrytych kluczowych punktów\n",
    "#         for keypoint, score in zip(keypoints, scores):\n",
    "#             #if score > 0.3:  # Próg ufności\n",
    "#                 x, y = int(keypoint[1] * frame.shape[1]), int(keypoint[0] * frame.shape[0])\n",
    "#                 cv2.circle(frame, (x, y), 5, (0, 255, 0), -1)\n",
    "\n",
    "#         # Wyświetlanie obrazu z wykrytymi kluczowymi punktami\n",
    "#     cv2.imshow('MoveNet Pose Detection', frame)\n",
    "#     key = cv2.waitKey(30)  # Adjust the delay as needed (milliseconds)\n",
    "#     last_frame = current_frame\n",
    "#     if key == 27:  # ESC key to exit\n",
    "#         break\n",
    "#     elif key == 83 or key == 100:\n",
    "#         current_frame += 1\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Movnet in HPE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "model_name = \"movenet_thunder\" #@param [\"movenet_lightning\", \"movenet_thunder\", \"movenet_lightning_f16.tflite\", \"movenet_thunder_f16.tflite\", \"movenet_lightning_int8.tflite\", \"movenet_thunder_int8.tflite\"]\n",
    "\n",
    "if \"tflite\" in model_name:\n",
    "  if \"movenet_lightning_f16\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite\n",
    "    input_size = 192\n",
    "  elif \"movenet_thunder_f16\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite\n",
    "    input_size = 256\n",
    "  elif \"movenet_lightning_int8\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4?lite-format=tflite\n",
    "    input_size = 192\n",
    "  elif \"movenet_thunder_int8\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/int8/4?lite-format=tflite\n",
    "    input_size = 256\n",
    "  else:\n",
    "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
    "\n",
    "  # Initialize the TFLite interpreter\n",
    "  interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "      input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "      coordinates and scores.\n",
    "    \"\"\"\n",
    "    # TF Lite format expects tensor type of uint8.\n",
    "    input_image = tf.cast(input_image, dtype=tf.uint8)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
    "    # Invoke inference.\n",
    "    interpreter.invoke()\n",
    "    # Get the model prediction.\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return keypoints_with_scores\n",
    "\n",
    "else:\n",
    "  if \"movenet_lightning\" in model_name:\n",
    "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
    "    input_size = 192\n",
    "  elif \"movenet_thunder\" in model_name:\n",
    "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
    "    input_size = 256\n",
    "  else:\n",
    "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
    "\n",
    "  def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "      input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "      coordinates and scores.\n",
    "    \"\"\"\n",
    "    model = module.signatures['serving_default']\n",
    "\n",
    "    # SavedModel format expects tensor type of int32.\n",
    "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "    # Run model inference.\n",
    "    outputs = model(input_image)\n",
    "    # Output is a [1, 1, 17, 3] tensor.\n",
    "    keypoints_with_scores = outputs['output_0'].numpy()\n",
    "    return keypoints_with_scores\n",
    "\n",
    "# Define the edges between keypoints to draw the skeleton\n",
    "KEYPOINT_EDGES = {\n",
    "    (0, 1): 'm', (0, 2): 'c', (1, 3): 'y', (2, 4): 'y',\n",
    "    (0, 5): 'm', (0, 6): 'c', (5, 7): 'm', (7, 9): 'y',\n",
    "    (6, 8): 'c', (8, 10): 'y', (5, 6): 'c', (5, 11): 'm',\n",
    "    (6, 12): 'c', (11, 12): 'y', (11, 13): 'm', (13, 15): 'y',\n",
    "    (12, 14): 'c', (14, 16): 'y'\n",
    "}\n",
    "\n",
    "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
    "    \"\"\"Draws the keypoints and the skeleton on the image.\"\"\"\n",
    "    y, x, _ = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y, x, 1]))\n",
    "\n",
    "    # Draw keypoints\n",
    "    for kp in shaped:\n",
    "        ky, kx, kp_conf = kp\n",
    "        if kp_conf > confidence_threshold:\n",
    "            cv2.circle(frame, (int(kx), int(ky)), 6, (0, 255, 0), -1)\n",
    "\n",
    "    # Draw skeleton\n",
    "    for edge, color in KEYPOINT_EDGES.items():\n",
    "        p1, p2 = edge\n",
    "        y1, x1, c1 = shaped[p1]\n",
    "        y2, x2, c2 = shaped[p2]\n",
    "\n",
    "        if c1 > confidence_threshold and c2 > confidence_threshold:\n",
    "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "#process_video(video_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Helper functions for visualization\n",
    "\n",
    "# Dictionary that maps from joint names to keypoint indices.\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# Maps bones to a matplotlib color name.\n",
    "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "\n",
    "def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
    "                                     height,\n",
    "                                     width,\n",
    "                                     keypoint_threshold=0.11):\n",
    "  \"\"\"Returns high confidence keypoints and edges for visualization.\n",
    "\n",
    "  Args:\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    height: height of the image in pixels.\n",
    "    width: width of the image in pixels.\n",
    "    keypoint_threshold: minimum confidence score for a keypoint to be\n",
    "      visualized.\n",
    "\n",
    "  Returns:\n",
    "    A (keypoints_xy, edges_xy, edge_colors) containing:\n",
    "      * the coordinates of all keypoints of all detected entities;\n",
    "      * the coordinates of all skeleton edges of all detected entities;\n",
    "      * the colors in which the edges should be plotted.\n",
    "  \"\"\"\n",
    "  keypoints_all = []\n",
    "  keypoint_edges_all = []\n",
    "  edge_colors = []\n",
    "  num_instances, _, _, _ = keypoints_with_scores.shape\n",
    "  for idx in range(num_instances):\n",
    "    kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
    "    kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
    "    kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
    "    kpts_absolute_xy = np.stack(\n",
    "        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
    "    kpts_above_thresh_absolute = kpts_absolute_xy[\n",
    "        kpts_scores > keypoint_threshold, :]\n",
    "    keypoints_all.append(kpts_above_thresh_absolute)\n",
    "\n",
    "    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
    "      if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
    "          kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
    "        x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
    "        y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
    "        x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
    "        y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
    "        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
    "        keypoint_edges_all.append(line_seg)\n",
    "        edge_colors.append(color)\n",
    "  if keypoints_all:\n",
    "    keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
    "  else:\n",
    "    keypoints_xy = np.zeros((0, 17, 2))\n",
    "\n",
    "  if keypoint_edges_all:\n",
    "    edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
    "  else:\n",
    "    edges_xy = np.zeros((0, 2, 2))\n",
    "  return keypoints_xy, edges_xy, edge_colors\n",
    "\n",
    "\n",
    "def draw_prediction_on_image(\n",
    "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
    "    output_image_height=None):\n",
    "  \"\"\"Draws the keypoint predictions on image.\n",
    "\n",
    "  Args:\n",
    "    image: A numpy array with shape [height, width, channel] representing the\n",
    "      pixel values of the input image.\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    crop_region: A dictionary that defines the coordinates of the bounding box\n",
    "      of the crop region in normalized coordinates (see the init_crop_region\n",
    "      function below for more detail). If provided, this function will also\n",
    "      draw the bounding box on the image.\n",
    "    output_image_height: An integer indicating the height of the output image.\n",
    "      Note that the image aspect ratio will be the same as the input image.\n",
    "\n",
    "  Returns:\n",
    "    A numpy array with shape [out_height, out_width, channel] representing the\n",
    "    image overlaid with keypoint predictions.\n",
    "  \"\"\"\n",
    "  height, width, channel = image.shape\n",
    "  aspect_ratio = float(width) / height\n",
    "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "  # To remove the huge white borders\n",
    "  fig.tight_layout(pad=0)\n",
    "  ax.margins(0)\n",
    "  ax.set_yticklabels([])\n",
    "  ax.set_xticklabels([])\n",
    "  plt.axis('off')\n",
    "\n",
    "  im = ax.imshow(image)\n",
    "  line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
    "  ax.add_collection(line_segments)\n",
    "  # Turn off tick labels\n",
    "  scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
    "\n",
    "  (keypoint_locs, keypoint_edges,\n",
    "   edge_colors) = _keypoints_and_edges_for_display(\n",
    "       keypoints_with_scores, height, width)\n",
    "\n",
    "  line_segments.set_segments(keypoint_edges)\n",
    "  line_segments.set_color(edge_colors)\n",
    "  if keypoint_edges.shape[0]:\n",
    "    line_segments.set_segments(keypoint_edges)\n",
    "    line_segments.set_color(edge_colors)\n",
    "  if keypoint_locs.shape[0]:\n",
    "    scat.set_offsets(keypoint_locs)\n",
    "\n",
    "  if crop_region is not None:\n",
    "    xmin = max(crop_region['x_min'] * width, 0.0)\n",
    "    ymin = max(crop_region['y_min'] * height, 0.0)\n",
    "    rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
    "    rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
    "    rect = patches.Rectangle(\n",
    "        (xmin,ymin),rec_width,rec_height,\n",
    "        linewidth=1,edgecolor='b',facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "  fig.canvas.draw()\n",
    "  image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "  image_from_plot = image_from_plot.reshape(\n",
    "      fig.canvas.get_width_height()[::-1] + (3,))\n",
    "  plt.close(fig)\n",
    "  if output_image_height is not None:\n",
    "    output_image_width = int(output_image_height / height * width)\n",
    "    image_from_plot = cv2.resize(\n",
    "        image_from_plot, dsize=(output_image_width, output_image_height),\n",
    "         interpolation=cv2.INTER_CUBIC)\n",
    "  return image_from_plot\n",
    "\n",
    "def to_gif(images, duration):\n",
    "  \"\"\"Converts image sequence (4D numpy array) to gif.\"\"\"\n",
    "  imageio.mimsave('./animation.gif', images, duration=duration)\n",
    "  return embed.embed_file('./animation.gif')\n",
    "\n",
    "def progress(value, max=100):\n",
    "  return HTML(\"\"\"\n",
    "      \n",
    "          {value}\n",
    "      \n",
    "  \"\"\".format(value=value, max=max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Trying to load a model of incompatible/unknown type. 'C:\\Users\\Damian\\AppData\\Local\\Temp\\tfhub_modules\\29fb93a7b6cbd61f33bf8c76777682f0a9b06839' contains neither 'saved_model.pb' nor 'saved_model.pbtxt'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-59ce150ee613>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;34m\"movenet_lightning\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://tfhub.dev/google/movenet/singlepose/lightning/4\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[0minput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m192\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[1;34m\"movenet_thunder\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow_hub\\module_v2.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(handle, tags, options)\u001b[0m\n\u001b[0;32m    111\u001b[0m   if (not tf.io.gfile.exists(saved_model_path) and\n\u001b[0;32m    112\u001b[0m       not tf.io.gfile.exists(saved_model_pbtxt_path)):\n\u001b[1;32m--> 113\u001b[1;33m     raise ValueError(\"Trying to load a model of incompatible/unknown type. \"\n\u001b[0m\u001b[0;32m    114\u001b[0m                      \u001b[1;34m\"'%s' contains neither '%s' nor '%s'.\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m                      (module_path, tf.saved_model.SAVED_MODEL_FILENAME_PB,\n",
      "\u001b[1;31mValueError\u001b[0m: Trying to load a model of incompatible/unknown type. 'C:\\Users\\Damian\\AppData\\Local\\Temp\\tfhub_modules\\29fb93a7b6cbd61f33bf8c76777682f0a9b06839' contains neither 'saved_model.pb' nor 'saved_model.pbtxt'."
     ]
    }
   ],
   "source": [
    "# model_name = \"movenet_lightning\" #@param [\"movenet_lightning\", \"movenet_thunder\", \"movenet_lightning_f16.tflite\", \"movenet_thunder_f16.tflite\", \"movenet_lightning_int8.tflite\", \"movenet_thunder_int8.tflite\"]\n",
    "\n",
    "# if \"tflite\" in model_name:\n",
    "#   if \"movenet_lightning_f16\" in model_name:\n",
    "#     !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite\n",
    "#     input_size = 192\n",
    "#   elif \"movenet_thunder_f16\" in model_name:\n",
    "#     !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite\n",
    "#     input_size = 256\n",
    "#   elif \"movenet_lightning_int8\" in model_name:\n",
    "#     !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4?lite-format=tflite\n",
    "#     input_size = 192\n",
    "#   elif \"movenet_thunder_int8\" in model_name:\n",
    "#     !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/int8/4?lite-format=tflite\n",
    "#     input_size = 256\n",
    "#   else:\n",
    "#     raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
    "\n",
    "#   # Initialize the TFLite interpreter\n",
    "#   interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "#   interpreter.allocate_tensors()\n",
    "\n",
    "#   def movenet(input_image):\n",
    "#     \"\"\"Runs detection on an input image.\n",
    "\n",
    "#     Args:\n",
    "#       input_image: A [1, height, width, 3] tensor represents the input image\n",
    "#         pixels. Note that the height/width should already be resized and match the\n",
    "#         expected input resolution of the model before passing into this function.\n",
    "\n",
    "#     Returns:\n",
    "#       A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "#       coordinates and scores.\n",
    "#     \"\"\"\n",
    "#     # TF Lite format expects tensor type of uint8.\n",
    "#     input_image = tf.cast(input_image, dtype=tf.uint8)\n",
    "#     input_details = interpreter.get_input_details()\n",
    "#     output_details = interpreter.get_output_details()\n",
    "#     interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
    "#     # Invoke inference.\n",
    "#     interpreter.invoke()\n",
    "#     # Get the model prediction.\n",
    "#     keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "#     return keypoints_with_scores\n",
    "\n",
    "# else:\n",
    "#   if \"movenet_lightning\" in model_name:\n",
    "#     module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
    "#     input_size = 192\n",
    "#   elif \"movenet_thunder\" in model_name:\n",
    "#     module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
    "#     input_size = 256\n",
    "#   else:\n",
    "#     raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
    "\n",
    "#   def movenet(input_image):\n",
    "#     \"\"\"Runs detection on an input image.\n",
    "\n",
    "#     Args:\n",
    "#       input_image: A [1, height, width, 3] tensor represents the input image\n",
    "#         pixels. Note that the height/width should already be resized and match the\n",
    "#         expected input resolution of the model before passing into this function.\n",
    "\n",
    "#     Returns:\n",
    "#       A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "#       coordinates and scores.\n",
    "#     \"\"\"\n",
    "#     model = module.signatures['serving_default']\n",
    "\n",
    "#     # SavedModel format expects tensor type of int32.\n",
    "#     input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "#     # Run model inference.\n",
    "#     outputs = model(input_image)\n",
    "#     # Output is a [1, 1, 17, 3] tensor.\n",
    "#     keypoints_with_scores = outputs['output_0'].numpy()\n",
    "#     return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Cropping Algorithm\n",
    "\n",
    "# Confidence score to determine whether a keypoint prediction is reliable.\n",
    "MIN_CROP_KEYPOINT_SCORE = 0.2\n",
    "\n",
    "def init_crop_region(image_height, image_width):\n",
    "  \"\"\"Defines the default crop region.\n",
    "\n",
    "  The function provides the initial crop region (pads the full image from both\n",
    "  sides to make it a square image) when the algorithm cannot reliably determine\n",
    "  the crop region from the previous frame.\n",
    "  \"\"\"\n",
    "  if image_width > image_height:\n",
    "    box_height = image_width / image_height\n",
    "    box_width = 1.0\n",
    "    y_min = (image_height / 2 - image_width / 2) / image_height\n",
    "    x_min = 0.0\n",
    "  else:\n",
    "    box_height = 1.0\n",
    "    box_width = image_height / image_width\n",
    "    y_min = 0.0\n",
    "    x_min = (image_width / 2 - image_height / 2) / image_width\n",
    "\n",
    "  return {\n",
    "    'y_min': y_min,\n",
    "    'x_min': x_min,\n",
    "    'y_max': y_min + box_height,\n",
    "    'x_max': x_min + box_width,\n",
    "    'height': box_height,\n",
    "    'width': box_width\n",
    "  }\n",
    "\n",
    "def torso_visible(keypoints):\n",
    "  \"\"\"Checks whether there are enough torso keypoints.\n",
    "\n",
    "  This function checks whether the model is confident at predicting one of the\n",
    "  shoulders/hips which is required to determine a good crop region.\n",
    "  \"\"\"\n",
    "  return ((keypoints[0, 0, KEYPOINT_DICT['left_hip'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE or\n",
    "          keypoints[0, 0, KEYPOINT_DICT['right_hip'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE) and\n",
    "          (keypoints[0, 0, KEYPOINT_DICT['left_shoulder'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE or\n",
    "          keypoints[0, 0, KEYPOINT_DICT['right_shoulder'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE))\n",
    "\n",
    "def determine_torso_and_body_range(\n",
    "    keypoints, target_keypoints, center_y, center_x):\n",
    "  \"\"\"Calculates the maximum distance from each keypoints to the center location.\n",
    "\n",
    "  The function returns the maximum distances from the two sets of keypoints:\n",
    "  full 17 keypoints and 4 torso keypoints. The returned information will be\n",
    "  used to determine the crop size. See determineCropRegion for more detail.\n",
    "  \"\"\"\n",
    "  torso_joints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']\n",
    "  max_torso_yrange = 0.0\n",
    "  max_torso_xrange = 0.0\n",
    "  for joint in torso_joints:\n",
    "    dist_y = abs(center_y - target_keypoints[joint][0])\n",
    "    dist_x = abs(center_x - target_keypoints[joint][1])\n",
    "    if dist_y > max_torso_yrange:\n",
    "      max_torso_yrange = dist_y\n",
    "    if dist_x > max_torso_xrange:\n",
    "      max_torso_xrange = dist_x\n",
    "\n",
    "  max_body_yrange = 0.0\n",
    "  max_body_xrange = 0.0\n",
    "  for joint in KEYPOINT_DICT.keys():\n",
    "    if keypoints[0, 0, KEYPOINT_DICT[joint], 2] < MIN_CROP_KEYPOINT_SCORE:\n",
    "      continue\n",
    "    dist_y = abs(center_y - target_keypoints[joint][0]);\n",
    "    dist_x = abs(center_x - target_keypoints[joint][1]);\n",
    "    if dist_y > max_body_yrange:\n",
    "      max_body_yrange = dist_y\n",
    "\n",
    "    if dist_x > max_body_xrange:\n",
    "      max_body_xrange = dist_x\n",
    "\n",
    "  return [max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange]\n",
    "\n",
    "def determine_crop_region(\n",
    "      keypoints, image_height,\n",
    "      image_width):\n",
    "  \"\"\"Determines the region to crop the image for the model to run inference on.\n",
    "\n",
    "  The algorithm uses the detected joints from the previous frame to estimate\n",
    "  the square region that encloses the full body of the target person and\n",
    "  centers at the midpoint of two hip joints. The crop size is determined by\n",
    "  the distances between each joints and the center point.\n",
    "  When the model is not confident with the four torso joint predictions, the\n",
    "  function returns a default crop which is the full image padded to square.\n",
    "  \"\"\"\n",
    "  target_keypoints = {}\n",
    "  for joint in KEYPOINT_DICT.keys():\n",
    "    target_keypoints[joint] = [\n",
    "      keypoints[0, 0, KEYPOINT_DICT[joint], 0] * image_height,\n",
    "      keypoints[0, 0, KEYPOINT_DICT[joint], 1] * image_width\n",
    "    ]\n",
    "\n",
    "  if torso_visible(keypoints):\n",
    "    center_y = (target_keypoints['left_hip'][0] +\n",
    "                target_keypoints['right_hip'][0]) / 2;\n",
    "    center_x = (target_keypoints['left_hip'][1] +\n",
    "                target_keypoints['right_hip'][1]) / 2;\n",
    "\n",
    "    (max_torso_yrange, max_torso_xrange,\n",
    "      max_body_yrange, max_body_xrange) = determine_torso_and_body_range(\n",
    "          keypoints, target_keypoints, center_y, center_x)\n",
    "\n",
    "    crop_length_half = np.amax(\n",
    "        [max_torso_xrange * 1.9, max_torso_yrange * 1.9,\n",
    "          max_body_yrange * 1.2, max_body_xrange * 1.2])\n",
    "\n",
    "    tmp = np.array(\n",
    "        [center_x, image_width - center_x, center_y, image_height - center_y])\n",
    "    crop_length_half = np.amin(\n",
    "        [crop_length_half, np.amax(tmp)]);\n",
    "\n",
    "    crop_corner = [center_y - crop_length_half, center_x - crop_length_half];\n",
    "\n",
    "    if crop_length_half > max(image_width, image_height) / 2:\n",
    "      return init_crop_region(image_height, image_width)\n",
    "    else:\n",
    "      crop_length = crop_length_half * 2;\n",
    "      return {\n",
    "        'y_min': crop_corner[0] / image_height,\n",
    "        'x_min': crop_corner[1] / image_width,\n",
    "        'y_max': (crop_corner[0] + crop_length) / image_height,\n",
    "        'x_max': (crop_corner[1] + crop_length) / image_width,\n",
    "        'height': (crop_corner[0] + crop_length) / image_height -\n",
    "            crop_corner[0] / image_height,\n",
    "        'width': (crop_corner[1] + crop_length) / image_width -\n",
    "            crop_corner[1] / image_width\n",
    "      }\n",
    "  else:\n",
    "    return init_crop_region(image_height, image_width)\n",
    "\n",
    "def crop_and_resize(image, crop_region, crop_size):\n",
    "  \"\"\"Crops and resize the image to prepare for the model input.\"\"\"\n",
    "  boxes=[[crop_region['y_min'], crop_region['x_min'],\n",
    "          crop_region['y_max'], crop_region['x_max']]]\n",
    "  output_image = tf.image.crop_and_resize(\n",
    "      image, box_indices=[0], boxes=boxes, crop_size=crop_size)\n",
    "  return output_image\n",
    "\n",
    "def run_inference(movenet, image, crop_region, crop_size):\n",
    "  \"\"\"Runs model inference on the cropped region.\n",
    "\n",
    "  The function runs the model inference on the cropped region and updates the\n",
    "  model output to the original image coordinate system.\n",
    "  \"\"\"\n",
    "  image_height, image_width, _ = image.shape\n",
    "  input_image = crop_and_resize(\n",
    "    tf.expand_dims(image, axis=0), crop_region, crop_size=crop_size)\n",
    "  # Run model inference.\n",
    "  keypoints_with_scores = movenet(input_image)\n",
    "  # Update the coordinates.\n",
    "  for idx in range(17):\n",
    "    keypoints_with_scores[0, 0, idx, 0] = (\n",
    "        crop_region['y_min'] * image_height +\n",
    "        crop_region['height'] * image_height *\n",
    "        keypoints_with_scores[0, 0, idx, 0]) / image_height\n",
    "    keypoints_with_scores[0, 0, idx, 1] = (\n",
    "        crop_region['x_min'] * image_width +\n",
    "        crop_region['width'] * image_width *\n",
    "        keypoints_with_scores[0, 0, idx, 1]) / image_width\n",
    "  return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "from IPython.display import HTML, display\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:<ipython-input-45-7b279b3e23e8>:162: MatplotlibDeprecationWarning: The tostring_rgb function was deprecated in Matplotlib 3.8 and will be removed two minor releases later. Use buffer_rgba instead.\n",
      "  image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from IPython.display import display, HTML,clear_output\n",
    "# from moviepy.editor import ImageSequenceClip\n",
    "# def to_mp4(output_images, filename, fps=30):\n",
    "#     clip = ImageSequenceClip([image[..., ::-1] for image in output_images], fps=fps)\n",
    "#     clip.write_videofile(filename, codec='libx264')\n",
    "\n",
    "# video_path = video_file\n",
    "# cap = cv2.VideoCapture(video_path)\n",
    "# num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "# fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "# image_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "# image_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "# crop_region = init_crop_region(image_height, image_width)\n",
    "# cv2.namedWindow('Video with Subtitles', cv2.WINDOW_NORMAL)\n",
    "# cv2.resizeWindow('Video with Subtitles', 800, 600)\n",
    "# output_images = []\n",
    "# bar = display(progress(0, num_frames-1), display_id=True)\n",
    "# last_frame = 0\n",
    "# frame_idx = 0\n",
    "# while cap.isOpened():\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "#     clear_output(wait=True)\n",
    "#     print(frame_idx)\n",
    "\n",
    "#     keypoints_with_scores = run_inference(\n",
    "#             movenet, frame, crop_region,\n",
    "#             crop_size=[input_size, input_size])\n",
    "#     img = draw_prediction_on_image(\n",
    "#             frame.astype(np.int32),\n",
    "#             keypoints_with_scores, crop_region=None,\n",
    "#             close_figure=True, output_image_height=300)\n",
    "\n",
    "#         #output_images.append(img)\n",
    "\n",
    "#         # Display the current image\n",
    "#     cv2.imshow('Video with Subtitles', img)\n",
    "\n",
    "\n",
    "#     crop_region = determine_crop_region(\n",
    "#             keypoints_with_scores, image_height, image_width)\n",
    "#     bar.update(progress(frame_idx, num_frames-1))\n",
    "#     last_frame = frame_idx\n",
    "#     key = cv2.waitKey(30)  # Adjust the delay as needed (milliseconds)\n",
    "\n",
    "#     if key == 27:  # ESC key to exit\n",
    "#         break\n",
    "#     elif key == 83 or key == 100:\n",
    "#         frame_idx += 1\n",
    "\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n",
    "# # Prepare MP4 visualization.\n",
    "# #to_mp4(output_images, 'output.mp4', fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:<ipython-input-45-7b279b3e23e8>:162: MatplotlibDeprecationWarning: The tostring_rgb function was deprecated in Matplotlib 3.8 and will be removed two minor releases later. Use buffer_rgba instead.\n",
      "  image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML,clear_output\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "def to_mp4(output_images, filename, fps=30):\n",
    "    clip = ImageSequenceClip([image[..., ::-1] for image in output_images], fps=fps)\n",
    "    clip.write_videofile(filename, codec='libx264')\n",
    "\n",
    "video_path = video_file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "image_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "image_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "crop_region = init_crop_region(image_height, image_width)\n",
    "cv2.namedWindow('Video with Subtitles', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('Video with Subtitles', 800, 600)\n",
    "output_images = []\n",
    "bar = display(progress(0, num_frames-1), display_id=True)\n",
    "last_frame = 0\n",
    "frame_idx = 0\n",
    "try:\n",
    "    while cap.isOpened():\n",
    "        #cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        clear_output(wait=True)\n",
    "        print(frame_idx)\n",
    "        #if last_frame!=frame_idx:\n",
    "        keypoints_with_scores = run_inference(\n",
    "                movenet, frame, crop_region,\n",
    "                crop_size=[input_size, input_size])\n",
    "        img = draw_prediction_on_image(\n",
    "                frame.astype(np.int32),\n",
    "                keypoints_with_scores, crop_region=None,\n",
    "                close_figure=True, output_image_height=300)\n",
    "\n",
    "            #output_images.append(img)\n",
    "\n",
    "            # Display the current image\n",
    "        cv2.imshow('Video with Subtitles', img)\n",
    "        crop_region = determine_crop_region(\n",
    "                keypoints_with_scores, image_height, image_width)\n",
    "        bar.update(progress(frame_idx, num_frames-1))\n",
    "        last_frame = frame_idx\n",
    "        key = cv2.waitKey(30)  # Adjust the delay as needed (milliseconds)\n",
    "\n",
    "        if key == 27:  # ESC key to exit\n",
    "            break\n",
    "        elif key == 83 or key == 100:\n",
    "            frame_idx += 1\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "# Prepare MP4 visualization.\n",
    "#to_mp4(output_images, 'output.mp4', fps=fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is optional tool to run main loop faster without processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we will approach the squat in two phases\n",
      "lewe ramie: \n",
      "x: -0.10126257\n",
      "y: -0.38646343\n",
      "z: 0.25926033\n",
      "visibility: 0.9979285\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.16045028\n",
      "y: -0.42304328\n",
      "z: 0.1250287\n",
      "visibility: 0.9999043\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.054906376\n",
      "y: 0.015925106\n",
      "z: 0.062126763\n",
      "visibility: 0.99943036\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.05485576\n",
      "y: -0.015774563\n",
      "z: -0.061007895\n",
      "visibility: 0.99995184\n",
      "\n",
      "kat na zdjeciu: \n",
      "-2.5454905818027704\n",
      "kat w 3d: \n",
      "-7.956737334590427\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.09328172\n",
      "y: -0.40218708\n",
      "z: 0.26579478\n",
      "visibility: 0.9968651\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.1601431\n",
      "y: -0.42502868\n",
      "z: 0.12521593\n",
      "visibility: 0.99986684\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.056926046\n",
      "y: 0.015943814\n",
      "z: 0.06404112\n",
      "visibility: 0.99944574\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.056657165\n",
      "y: -0.01579237\n",
      "z: -0.0630135\n",
      "visibility: 0.9999524\n",
      "\n",
      "kat na zdjeciu: \n",
      "-7.048258708727123\n",
      "kat w 3d: \n",
      "-5.150247842948468\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.076876715\n",
      "y: -0.41313618\n",
      "z: 0.2614864\n",
      "visibility: 0.9963455\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.16003326\n",
      "y: -0.4452573\n",
      "z: 0.08232707\n",
      "visibility: 0.99986684\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.057429407\n",
      "y: 0.016067546\n",
      "z: 0.07105132\n",
      "visibility: 0.99947464\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.05713428\n",
      "y: -0.01591855\n",
      "z: -0.07002735\n",
      "visibility: 0.99995464\n",
      "\n",
      "kat na zdjeciu: \n",
      "-17.239750339258457\n",
      "kat w 3d: \n",
      "-7.721288152695271\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.0765571\n",
      "y: -0.4111862\n",
      "z: 0.29813915\n",
      "visibility: 0.99614674\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.15808244\n",
      "y: -0.45297092\n",
      "z: 0.1381455\n",
      "visibility: 0.99986607\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.060340047\n",
      "y: 0.017135104\n",
      "z: 0.073723435\n",
      "visibility: 0.99950373\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.059919916\n",
      "y: -0.01700896\n",
      "z: -0.072867244\n",
      "visibility: 0.9999547\n",
      "\n",
      "kat na zdjeciu: \n",
      "-28.616073658782867\n",
      "kat w 3d: \n",
      "-10.097412058827414\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.100091584\n",
      "y: -0.37942824\n",
      "z: 0.310653\n",
      "visibility: 0.99583054\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.15525451\n",
      "y: -0.45327765\n",
      "z: 0.19236664\n",
      "visibility: 0.9998616\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.06487913\n",
      "y: 0.018016076\n",
      "z: 0.07360966\n",
      "visibility: 0.9995268\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.06440798\n",
      "y: -0.017915746\n",
      "z: -0.07280668\n",
      "visibility: 0.9999541\n",
      "\n",
      "kat na zdjeciu: \n",
      "-25.752326449742263\n",
      "kat w 3d: \n",
      "-16.130556323859402\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.1141155\n",
      "y: -0.33961204\n",
      "z: 0.37604585\n",
      "visibility: 0.9942719\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.15304142\n",
      "y: -0.43956098\n",
      "z: 0.23445208\n",
      "visibility: 0.99984616\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.06803705\n",
      "y: 0.02031709\n",
      "z: 0.07500566\n",
      "visibility: 0.9995062\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.06739911\n",
      "y: -0.02022895\n",
      "z: -0.07435185\n",
      "visibility: 0.99993545\n",
      "\n",
      "kat na zdjeciu: \n",
      "-29.089031382271767\n",
      "kat w 3d: \n",
      "-20.511865083155453\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "first unloaded to solve problems\n",
      "lewe ramie: \n",
      "x: -0.13194542\n",
      "y: -0.34072623\n",
      "z: 0.39672312\n",
      "visibility: 0.9944426\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.14131925\n",
      "y: -0.4457674\n",
      "z: 0.20953271\n",
      "visibility: 0.9998591\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.069667384\n",
      "y: 0.025800874\n",
      "z: 0.084665105\n",
      "visibility: 0.9995044\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.06901815\n",
      "y: -0.025842244\n",
      "z: -0.08389709\n",
      "visibility: 0.99992776\n",
      "\n",
      "kat na zdjeciu: \n",
      "-37.43740187719416\n",
      "kat w 3d: \n",
      "-21.02643843455229\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.13187824\n",
      "y: -0.34631017\n",
      "z: 0.36698553\n",
      "visibility: 0.9946173\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.13833581\n",
      "y: -0.4555064\n",
      "z: 0.17744844\n",
      "visibility: 0.99986994\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.0704717\n",
      "y: 0.02594877\n",
      "z: 0.086306915\n",
      "visibility: 0.99951845\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.069884926\n",
      "y: -0.026026156\n",
      "z: -0.08546049\n",
      "visibility: 0.99992895\n",
      "\n",
      "kat na zdjeciu: \n",
      "-38.44713655844908\n",
      "kat w 3d: \n",
      "-22.00413451127726\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.14394428\n",
      "y: -0.34937736\n",
      "z: 0.3657746\n",
      "visibility: 0.99490654\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.1369972\n",
      "y: -0.46149307\n",
      "z: 0.13284118\n",
      "visibility: 0.99988085\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.07289612\n",
      "y: 0.0262742\n",
      "z: 0.09002675\n",
      "visibility: 0.9995432\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.07247903\n",
      "y: -0.026414724\n",
      "z: -0.088959046\n",
      "visibility: 0.99993217\n",
      "\n",
      "kat na zdjeciu: \n",
      "-39.29613732917238\n",
      "kat w 3d: \n",
      "-21.755529361894684\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.15476674\n",
      "y: -0.35209605\n",
      "z: 0.36759412\n",
      "visibility: 0.99470663\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.1374026\n",
      "y: -0.46108255\n",
      "z: 0.13559796\n",
      "visibility: 0.99988496\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.07505811\n",
      "y: 0.027084261\n",
      "z: 0.092273794\n",
      "visibility: 0.99951637\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.07467338\n",
      "y: -0.027325867\n",
      "z: -0.09112756\n",
      "visibility: 0.9999198\n",
      "\n",
      "kat na zdjeciu: \n",
      "-36.65632528401197\n",
      "kat w 3d: \n",
      "-20.456778276781588\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.17050105\n",
      "y: -0.3360422\n",
      "z: 0.4001146\n",
      "visibility: 0.9943454\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.13775775\n",
      "y: -0.4600333\n",
      "z: 0.1515186\n",
      "visibility: 0.9998826\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.07772521\n",
      "y: 0.030505715\n",
      "z: 0.093715385\n",
      "visibility: 0.99948865\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.07727903\n",
      "y: -0.030878333\n",
      "z: -0.09257224\n",
      "visibility: 0.9998906\n",
      "\n",
      "kat na zdjeciu: \n",
      "-34.25229859371963\n",
      "kat w 3d: \n",
      "-21.911497179494408\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.16523361\n",
      "y: -0.33901304\n",
      "z: 0.3844\n",
      "visibility: 0.99430287\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.13445717\n",
      "y: -0.4563381\n",
      "z: 0.17459443\n",
      "visibility: 0.9998828\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.07792353\n",
      "y: 0.030427223\n",
      "z: 0.093059935\n",
      "visibility: 0.99948394\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.07743263\n",
      "y: -0.030767035\n",
      "z: -0.09194526\n",
      "visibility: 0.99988294\n",
      "\n",
      "kat na zdjeciu: \n",
      "-42.268264800425605\n",
      "kat w 3d: \n",
      "-21.379700902526306\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.16023809\n",
      "y: -0.34894457\n",
      "z: 0.29050946\n",
      "visibility: 0.9941624\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.11849353\n",
      "y: -0.4666646\n",
      "z: 0.11570386\n",
      "visibility: 0.99988073\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.07466106\n",
      "y: 0.030356996\n",
      "z: 0.0913536\n",
      "visibility: 0.9994976\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.07414639\n",
      "y: -0.030692082\n",
      "z: -0.090273514\n",
      "visibility: 0.99988705\n",
      "\n",
      "kat na zdjeciu: \n",
      "-40.671163229985794\n",
      "kat w 3d: \n",
      "-22.89637064370906\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.16144367\n",
      "y: -0.3502231\n",
      "z: 0.26715845\n",
      "visibility: 0.99442697\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.120319426\n",
      "y: -0.46543896\n",
      "z: 0.11978092\n",
      "visibility: 0.9998865\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.07465349\n",
      "y: 0.029423598\n",
      "z: 0.08918796\n",
      "visibility: 0.99953026\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.07416207\n",
      "y: -0.02973722\n",
      "z: -0.08809395\n",
      "visibility: 0.99989617\n",
      "\n",
      "kat na zdjeciu: \n",
      "-36.646381648997924\n",
      "kat w 3d: \n",
      "-22.24016501320491\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.16262844\n",
      "y: -0.35441726\n",
      "z: 0.2565874\n",
      "visibility: 0.99449384\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.13754489\n",
      "y: -0.45300168\n",
      "z: 0.095095016\n",
      "visibility: 0.99987924\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.080938935\n",
      "y: 0.02721908\n",
      "z: 0.08893601\n",
      "visibility: 0.9995549\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.08050819\n",
      "y: -0.027672479\n",
      "z: -0.08786179\n",
      "visibility: 0.99990284\n",
      "\n",
      "kat na zdjeciu: \n",
      "-24.89527668462331\n",
      "kat w 3d: \n",
      "-18.18147114402404\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.124212526\n",
      "y: -0.35455093\n",
      "z: 0.23396164\n",
      "visibility: 0.99465686\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.12914644\n",
      "y: -0.42783082\n",
      "z: 0.042759355\n",
      "visibility: 0.99987173\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.07658839\n",
      "y: 0.026279861\n",
      "z: 0.090692036\n",
      "visibility: 0.9995819\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.07624371\n",
      "y: -0.026770031\n",
      "z: -0.08953076\n",
      "visibility: 0.9999103\n",
      "\n",
      "kat na zdjeciu: \n",
      "-20.48455145694086\n",
      "kat w 3d: \n",
      "-16.13163759463474\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.020430729\n",
      "y: -0.40614286\n",
      "z: 0.002705035\n",
      "visibility: 0.9949655\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.036928218\n",
      "y: -0.38709038\n",
      "z: -0.18749608\n",
      "visibility: 0.999825\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.02208315\n",
      "y: 0.0051230555\n",
      "z: 0.09307495\n",
      "visibility: 0.99958587\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.021923099\n",
      "y: -0.005258098\n",
      "z: -0.09205825\n",
      "visibility: 0.99991584\n",
      "\n",
      "kat na zdjeciu: \n",
      "54.64564824148336\n",
      "kat w 3d: \n",
      "18.37453832529116\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.11775714\n",
      "y: -0.35964587\n",
      "z: 0.008451517\n",
      "visibility: 0.9954336\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.09161473\n",
      "y: -0.40940005\n",
      "z: -0.05903734\n",
      "visibility: 0.9998413\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.048966903\n",
      "y: 0.0074199396\n",
      "z: 0.09290902\n",
      "visibility: 0.99962187\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.049221598\n",
      "y: -0.0077182627\n",
      "z: -0.091860816\n",
      "visibility: 0.9999236\n",
      "\n",
      "kat na zdjeciu: \n",
      "-36.93052575681051\n",
      "kat w 3d: \n",
      "-13.367562867619627\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.11813898\n",
      "y: -0.303647\n",
      "z: 0.21121873\n",
      "visibility: 0.99553084\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.10026434\n",
      "y: -0.4103374\n",
      "z: 0.023742907\n",
      "visibility: 0.99984574\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.053472657\n",
      "y: 0.017503832\n",
      "z: 0.09457221\n",
      "visibility: 0.99964744\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.053546\n",
      "y: -0.017751656\n",
      "z: -0.09348712\n",
      "visibility: 0.9999299\n",
      "\n",
      "kat na zdjeciu: \n",
      "-35.231962078742306\n",
      "kat w 3d: \n",
      "-26.03558465337624\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.12464206\n",
      "y: -0.30815887\n",
      "z: 0.24176565\n",
      "visibility: 0.9958772\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.10939584\n",
      "y: -0.41150123\n",
      "z: 0.04420909\n",
      "visibility: 0.99985754\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.0703307\n",
      "y: 0.0191581\n",
      "z: 0.09521612\n",
      "visibility: 0.99967825\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.0700932\n",
      "y: -0.019415684\n",
      "z: -0.094082646\n",
      "visibility: 0.99993646\n",
      "\n",
      "kat na zdjeciu: \n",
      "-34.2625324310777\n",
      "kat w 3d: \n",
      "-23.824454098319467\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.11951274\n",
      "y: -0.30550393\n",
      "z: 0.26491585\n",
      "visibility: 0.99606836\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.11002854\n",
      "y: -0.3974104\n",
      "z: 0.0494761\n",
      "visibility: 0.99986\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.07072567\n",
      "y: 0.021194901\n",
      "z: 0.09868227\n",
      "visibility: 0.9997055\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.0704427\n",
      "y: -0.021449864\n",
      "z: -0.097440556\n",
      "visibility: 0.9999423\n",
      "\n",
      "kat na zdjeciu: \n",
      "-26.97174478477027\n",
      "kat w 3d: \n",
      "-21.8207616213025\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.13718246\n",
      "y: -0.26394585\n",
      "z: 0.3207793\n",
      "visibility: 0.99634945\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.13133354\n",
      "y: -0.39535046\n",
      "z: 0.12262999\n",
      "visibility: 0.99986684\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.079072945\n",
      "y: 0.029077442\n",
      "z: 0.09949128\n",
      "visibility: 0.99973\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.0787448\n",
      "y: -0.029344885\n",
      "z: -0.098323196\n",
      "visibility: 0.9999471\n",
      "\n",
      "kat na zdjeciu: \n",
      "-35.8453357495016\n",
      "kat w 3d: \n",
      "-26.07589953002564\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.15346165\n",
      "y: -0.27743986\n",
      "z: 0.31949133\n",
      "visibility: 0.9965626\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.14030759\n",
      "y: -0.4038222\n",
      "z: 0.13064012\n",
      "visibility: 0.9998736\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.08360521\n",
      "y: 0.031298082\n",
      "z: 0.10018994\n",
      "visibility: 0.9997398\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.083325624\n",
      "y: -0.031528044\n",
      "z: -0.09906325\n",
      "visibility: 0.9999495\n",
      "\n",
      "kat na zdjeciu: \n",
      "-35.03443432017773\n",
      "kat w 3d: \n",
      "-23.277839816730904\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.15709452\n",
      "y: -0.28209847\n",
      "z: 0.31846032\n",
      "visibility: 0.9967729\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.14625089\n",
      "y: -0.40717766\n",
      "z: 0.12286549\n",
      "visibility: 0.99988216\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.08570302\n",
      "y: 0.030878967\n",
      "z: 0.09833244\n",
      "visibility: 0.9997568\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.08543424\n",
      "y: -0.031132702\n",
      "z: -0.09720408\n",
      "visibility: 0.99995345\n",
      "\n",
      "kat na zdjeciu: \n",
      "-30.92613396130846\n",
      "kat w 3d: \n",
      "-22.40794753491677\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.1686664\n",
      "y: -0.30226952\n",
      "z: 0.30794176\n",
      "visibility: 0.9969304\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.1453505\n",
      "y: -0.41837615\n",
      "z: 0.11764114\n",
      "visibility: 0.9998896\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.0858672\n",
      "y: 0.030230783\n",
      "z: 0.09633758\n",
      "visibility: 0.999764\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.085622296\n",
      "y: -0.030528262\n",
      "z: -0.09514744\n",
      "visibility: 0.9999561\n",
      "\n",
      "kat na zdjeciu: \n",
      "-32.66935462663231\n",
      "kat w 3d: \n",
      "-20.29169426348738\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.17480929\n",
      "y: -0.32924855\n",
      "z: 0.25788528\n",
      "visibility: 0.99662715\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.14954424\n",
      "y: -0.42415416\n",
      "z: 0.10947652\n",
      "visibility: 0.99988925\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.08622491\n",
      "y: 0.02821697\n",
      "z: 0.09147979\n",
      "visibility: 0.9997321\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.085969076\n",
      "y: -0.028564895\n",
      "z: -0.090225235\n",
      "visibility: 0.99995404\n",
      "\n",
      "kat na zdjeciu: \n",
      "-26.778509349177433\n",
      "kat w 3d: \n",
      "-16.309438721798735\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.17460522\n",
      "y: -0.33688638\n",
      "z: 0.25949055\n",
      "visibility: 0.99649155\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.12516634\n",
      "y: -0.42873153\n",
      "z: 0.09689609\n",
      "visibility: 0.9998901\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.0837218\n",
      "y: 0.027674181\n",
      "z: 0.09141054\n",
      "visibility: 0.999724\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.083447866\n",
      "y: -0.02798203\n",
      "z: -0.09015646\n",
      "visibility: 0.99995315\n",
      "\n",
      "kat na zdjeciu: \n",
      "-27.055430303297697\n",
      "kat w 3d: \n",
      "-17.034216665118457\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.18526685\n",
      "y: -0.33563828\n",
      "z: 0.2530146\n",
      "visibility: 0.9965424\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.12400338\n",
      "y: -0.43438607\n",
      "z: 0.108341515\n",
      "visibility: 0.99989694\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.08425768\n",
      "y: 0.027601445\n",
      "z: 0.09032708\n",
      "visibility: 0.99968415\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.08400692\n",
      "y: -0.027916128\n",
      "z: -0.089055255\n",
      "visibility: 0.99994963\n",
      "\n",
      "kat na zdjeciu: \n",
      "-20.148314271583743\n",
      "kat w 3d: \n",
      "-17.707913551476903\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.20353909\n",
      "y: -0.32762164\n",
      "z: 0.28515127\n",
      "visibility: 0.9966689\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.11816419\n",
      "y: -0.4354362\n",
      "z: 0.14467792\n",
      "visibility: 0.9999034\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.08594536\n",
      "y: 0.028089583\n",
      "z: 0.09032075\n",
      "visibility: 0.99963075\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.08566146\n",
      "y: -0.028414788\n",
      "z: -0.08905535\n",
      "visibility: 0.99994063\n",
      "\n",
      "kat na zdjeciu: \n",
      "-20.999364005093714\n",
      "kat w 3d: \n",
      "-18.527887319103513\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.19708247\n",
      "y: -0.31405783\n",
      "z: 0.3408179\n",
      "visibility: 0.9962824\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.11753429\n",
      "y: -0.41692072\n",
      "z: 0.18607071\n",
      "visibility: 0.99989015\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.08584784\n",
      "y: 0.02819083\n",
      "z: 0.08968738\n",
      "visibility: 0.9992148\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.085587546\n",
      "y: -0.028498461\n",
      "z: -0.08855966\n",
      "visibility: 0.9998583\n",
      "\n",
      "kat na zdjeciu: \n",
      "-23.41353246249536\n",
      "kat w 3d: \n",
      "-18.10498294076108\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.19303878\n",
      "y: -0.31473556\n",
      "z: 0.32910997\n",
      "visibility: 0.9963501\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.11061564\n",
      "y: -0.44489953\n",
      "z: 0.1674001\n",
      "visibility: 0.9999001\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.08501947\n",
      "y: 0.0302432\n",
      "z: 0.08944267\n",
      "visibility: 0.999151\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.08479226\n",
      "y: -0.030483283\n",
      "z: -0.088395536\n",
      "visibility: 0.999859\n",
      "\n",
      "kat na zdjeciu: \n",
      "-26.356067970138398\n",
      "kat w 3d: \n",
      "-23.202792410723358\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.19416198\n",
      "y: -0.29887035\n",
      "z: 0.32791743\n",
      "visibility: 0.9960693\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.10188577\n",
      "y: -0.4439118\n",
      "z: 0.16107726\n",
      "visibility: 0.9999049\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.079873376\n",
      "y: 0.03703097\n",
      "z: 0.088772826\n",
      "visibility: 0.9990263\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.07973184\n",
      "y: -0.037167374\n",
      "z: -0.08780396\n",
      "visibility: 0.99984163\n",
      "\n",
      "kat na zdjeciu: \n",
      "-33.120366410443296\n",
      "kat w 3d: \n",
      "-26.10142614048082\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.20043455\n",
      "y: -0.29319763\n",
      "z: 0.32466233\n",
      "visibility: 0.9957704\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.09282601\n",
      "y: -0.45208806\n",
      "z: 0.17112534\n",
      "visibility: 0.99991137\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.0740608\n",
      "y: 0.0437623\n",
      "z: 0.0831483\n",
      "visibility: 0.9987284\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.07392531\n",
      "y: -0.043714862\n",
      "z: -0.082296275\n",
      "visibility: 0.9998273\n",
      "\n",
      "kat na zdjeciu: \n",
      "-46.64036336618885\n",
      "kat w 3d: \n",
      "-28.449114942072374\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.19946754\n",
      "y: -0.29406196\n",
      "z: 0.319928\n",
      "visibility: 0.9956308\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.086489595\n",
      "y: -0.46009764\n",
      "z: 0.16936266\n",
      "visibility: 0.9999166\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.069486156\n",
      "y: 0.04578289\n",
      "z: 0.080514885\n",
      "visibility: 0.9987023\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.06946238\n",
      "y: -0.0456602\n",
      "z: -0.07970017\n",
      "visibility: 0.9998338\n",
      "\n",
      "kat na zdjeciu: \n",
      "-40.462531786642884\n",
      "kat w 3d: \n",
      "-30.140794416074996\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n",
      "lewe ramie: \n",
      "x: -0.19729598\n",
      "y: -0.30254117\n",
      "z: 0.233457\n",
      "visibility: 0.99559826\n",
      "\n",
      "prawe ramie: \n",
      "x: 0.073314786\n",
      "y: -0.4668951\n",
      "z: 0.0793155\n",
      "visibility: 0.9999229\n",
      "\n",
      "lewe biodro: \n",
      "x: -0.059499845\n",
      "y: 0.045222275\n",
      "z: 0.07905733\n",
      "visibility: 0.9987082\n",
      "\n",
      "prawe biodro: \n",
      "x: 0.059788693\n",
      "y: -0.04514256\n",
      "z: -0.078037724\n",
      "visibility: 0.99984473\n",
      "\n",
      "kat na zdjeciu: \n",
      "-36.252798162656546\n",
      "kat w 3d: \n",
      "-31.272168982556277\n",
      "przod:\n",
      "False\n",
      "lewo:\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(video_file)\n",
    "dq = collections.deque()\n",
    "cv2.namedWindow('Video with Subtitles', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('Video with Subtitles', 800, 600)\n",
    "last_frame = 0 \n",
    "current_frame = 0 \n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "curr_sub_start = 0\n",
    "import os\n",
    "clear = lambda: os.system('cls')\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
    "        current_time = current_frame / fps\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "        if last_frame != current_frame:\n",
    "            while subtitles[curr_sub_start]['start'] < current_time:\n",
    "                print(subtitles[curr_sub_start]['text'])\n",
    "                dq.append(curr_sub_start)\n",
    "                curr_sub_start += 1\n",
    "            if len(dq) > 0:\n",
    "                while subtitles[dq[0]]['start'] + subtitles[dq[0]]['duration'] < current_time:\n",
    "                    dq.popleft()\n",
    "\n",
    "            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            edges = cv2.Canny(gray_frame, threshold1=100, threshold2=200)  \n",
    "\n",
    "            sub_index = 0\n",
    "            for x in dq:\n",
    "                cv2.putText(frame, subtitles[x]['text'], (50, 50 + 50 * sub_index), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                sub_index += 1\n",
    "\n",
    "            #img = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "            detection_result = detector.process(frame)\n",
    "            if(detection_result.pose_landmarks):\n",
    "                clear()\n",
    "                print(\"lewe ramie: \")\n",
    "                print(detection_result.pose_world_landmarks.landmark[11])\n",
    "                print(\"prawe ramie: \")\n",
    "                print(detection_result.pose_world_landmarks.landmark[12])\n",
    "                print(\"lewe biodro: \")\n",
    "                print(detection_result.pose_world_landmarks.landmark[23])\n",
    "                print(\"prawe biodro: \")\n",
    "                print(detection_result.pose_world_landmarks.landmark[24])\n",
    "                #print(detection_result.pose_landmarks[11])\n",
    "                body_angle = calculate_body_rotation_angle(detection_result.pose_landmarks.landmark)\n",
    "                body_angle_3d = calculate_body_rotation_angle(detection_result.pose_world_landmarks.landmark)\n",
    "                print(\"kat na zdjeciu: \")\n",
    "                print(body_angle)\n",
    "                print(\"kat w 3d: \")\n",
    "                print(body_angle_3d)\n",
    "\n",
    "                # wheter human is front or back to camera \n",
    "                front = False if detection_result.pose_world_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP].x > 0 else True\n",
    "                left = True if detection_result.pose_world_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP].z < detection_result.pose_world_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP].z else False\n",
    "                print(\"przod:\")\n",
    "                print(front)\n",
    "                print(\"lewo:\")\n",
    "                print(front)\n",
    "                #cv2.putText(frame, body_angle, (50, 500), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                annotated_image = draw_landmarks_on_image(frame, detection_result)\n",
    "                bgr_image = cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR)\n",
    "                cv2.imshow('Video with Subtitles', bgr_image)\n",
    "            else:\n",
    "               cv2.imshow('Video with Subtitles', frame) \n",
    "\n",
    "        #cv2.imshow('Video with Subtitles', frame)\n",
    "\n",
    "        #cv2.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))    # Wait for user input (right arrow key to go to the next frame)\n",
    "        key = cv2.waitKey(30)  # Adjust the delay as needed (milliseconds)\n",
    "        last_frame = current_frame\n",
    "        if key == 27:  # ESC key to exit\n",
    "            break\n",
    "        elif key == 83 or key == 100:\n",
    "            current_frame += 1\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### divide text into sentences and add punctuation with ml model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4705\n",
      "we will approach the squat in two phasesfirst unloaded to solve problemsassociated with the bottom position andthen loaded to learn how to apply thebottom position to the hip drive usedfor heavier weights since the majorityof the problems with the squat happenedat the bottom this method expedites theprocess quite effectively we will use afairly neutral foot placement with theheels about shoulder width apart and thetoes pointed out at about 30 degreesmany people will assume a stance withtoes pointed too forward so you may needto point them out more than you want tonext you're going to assume the positionyou will be in at the bottom of a squatwithout the barsquat down all the way to a position inwhich the apex of the hip crease dropsjust below the top of the patella putyour elbows against your knees with thepalms of your hands together and shoveyour knees out notice your feet are flaton the floor your knees are shoved outto where they are in a parallel linewith your feet and just a little infront of your toes your back should beas flat as you can get it also noticethat your back is inclined at about a 45degree angle not at all vertical andyour eyes are looking down at the floora few feet in front of you after you'veestablished the bottom position come upout of the bottom by driving your buttstraight up in the air up not forwardnot back this movement keeps your weightsolidly over the whole foot instead ofletting it shift to the toes think abouta chain hook to your hips pulling youstraight up out of the bottom set therack height so that the bar is at aboutthe level of your mid sternum take aneven grip on the bar measured from themarkings placed on the bar for thispurpose a standard power bar has 16 to17 inches between the ends of the insideneural and 32 inches between the fingermarks grip width for the squat will varywith shoulder width and flexibility butin general the hands will be betweenthese two markings with the narrowestgrip you can manage a narrower gripallows a flexible person to bettersupport the bar with the posteriormuscles of the shoulders and a widergrip allows an inflexible person to getmore comfortable under the bar thethumbs should be placed on top of thebar so that the wrists can be held in astraight line with the forearms theelbows should be lifted up to trap thebar between the hands and the backelbows should be up but not high withyour grip in place and your hands andthumbs on top of the bar dip your headunder the bar and come up into positionwith the bar on your back just below thespine of the scapula the bone you feelat the top of the shoulder blades andthen secure it in place by lifting yourelbows and chest at the same time itshould feel as though the bar is restingon a shelf under the traps and on top ofthe posterior deltoids take the bar outof the rack in the same position inwhich it is to be squatted with thetorso and shoulders tight the chest andelbows up the head position down andboth feet under the bar step back justenough to clear the rack and assume thesame stance you used earlier again heelsshould be about shoulder width apartwith toes pointed out about 30 degreesat this point you are ready to squatwith the empty bar everything you'reabout to do is the same as you didunweighted only two things are differentone you don't have your elbows availableto help push your knees out so you needto do this with your brain and two don'tstop at the bottom just go down andimmediately come back up driving yourbutt straight up not forward not backout of the bottom now look down at aspot on the floor about four to fivefeet in front of you take a big breathand hold it and squat you should be ingood balance at the bottom of the squatwith your weight balanced evenly overyour feetneither on your heels nor forward onyour toesbalance problems usually indicate a backangle that is too verticalremember that the back angle will not bevertical at allsit back lean forward shove your kneesout point your nipples at the floorallow your hips to perform the squat notyour legs do not accept anything lessthan full depth ever if you are high itis usually because your knees are notout most people who have problems withthe squat do not shove their knees outenough do a set of five and rack the barwalk forward until the bar touches thevertical parts of the rack find theuprights not the hooks you can't missthe uprights and if you touch themyou'll be over the hooks the generalplan is to do a couple more sets of 5reps with the empty bar to nail down themovement pattern and then add weight doanother set of 5 and keep increasing ineven increments until the next increasewould compromise your form and that isthe first squat workout[Music]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Damian\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Damian\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Damian\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\pipelines\\token_classification.py:168: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.NONE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4790\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text =''\n",
    "for obj in subtitles:\n",
    "    text+=obj['text']\n",
    "\n",
    "print(len(text))\n",
    "print(text)\n",
    "\n",
    "\n",
    "from deepmultilingualpunctuation import PunctuationModel\n",
    "model = PunctuationModel()\n",
    "\n",
    "result = model.restore_punctuation(text)\n",
    "print(len(result))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing artificial connections in words (auto generating subtitles from yt isn't ideal)\n",
    "### also pos-tags are added here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = nltk.sent_tokenize(result)\n",
    "\n",
    "import wordsegment\n",
    "from wordsegment import load, segment\n",
    "load()\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# segment powoduje również tokenizacje zdania dlatego ten etap(tokenizacji) zostanie pominięty.\n",
    "sents = [(segment(sent)) for sent in sents]\n",
    "#porter = nltk.PorterStemmer()\n",
    "#sents = [[porter.stem(t) for t in sent] for sent in sents]\n",
    "sents = [nltk.pos_tag(sent) for sent in sents]\n",
    "grammar = r\"\"\"\n",
    "  NP: {<DT|PP\\$>?<JJ>*<NN>} \n",
    "      {<NNP>+}               \n",
    "\"\"\"\n",
    "# grammar = r\"\"\"\n",
    "#   NP: {<DT>?<JJ>*<NN>}\n",
    "#   VP: {<VB.*><NP|PP>*}\n",
    "#   PP: {<IN><NP>}\n",
    "#   ADJP: {<JJ>}\n",
    "#   ADVP: {<RB.*>}\n",
    "# \"\"\"\n",
    "cp = nltk.RegexpParser(grammar) \n",
    "\t\n",
    "# class ConsecutiveNPChunkTagger(nltk.TaggerI): \n",
    "\n",
    "#     def __init__(self, train_sents):\n",
    "#         train_set = []\n",
    "#         for tagged_sent in train_sents:\n",
    "#             untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "#             history = []\n",
    "#             for i, (word, tag) in enumerate(tagged_sent):\n",
    "#                 featureset = npchunk_features(untagged_sent, i, history) \n",
    "#                 train_set.append( (featureset, tag) )\n",
    "#                 history.append(tag)\n",
    "#         self.classifier = nltk.MaxentClassifier.train( \n",
    "#             train_set, algorithm='megam', trace=0)\n",
    "\n",
    "#     def tag(self, sentence):\n",
    "#         history = []\n",
    "#         for i, word in enumerate(sentence):\n",
    "#             featureset = npchunk_features(sentence, i, history)\n",
    "#             tag = self.classifier.classify(featureset)\n",
    "#             history.append(tag)\n",
    "#         return zip(sentence, history)\n",
    "\n",
    "# class ConsecutiveNPChunker(nltk.ChunkParserI):\n",
    "#     def __init__(self, train_sents):\n",
    "#         tagged_sents = [[((w,t),c) for (w,t,c) in\n",
    "#                          nltk.chunk.tree2conlltags(sent)]\n",
    "#                         for sent in train_sents]\n",
    "#         self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n",
    "\n",
    "#     def parse(self, sentence):\n",
    "#         tagged_sents = self.tagger.tag(sentence)\n",
    "#         conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
    "#         return nltk.chunk.conlltags2tree(conlltags)\n",
    "    \n",
    "# def npchunk_features(sentence, i, history):\n",
    "#      word, pos = sentence[i]\n",
    "#      return {\"pos\": pos}\n",
    "# chunker = ConsecutiveNPChunker(train_sents)\n",
    "# print(chunker.evaluate(test_sents))\n",
    "\n",
    "\n",
    "# sents = [cp.parse(sent) for sent in sents]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('measured', 'VBN'), ('from', 'IN'), ('the', 'DT'), ('markings', 'NNS'), ('placed', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('bar', 'NN'), ('for', 'IN'), ('this', 'DT'), ('purpose', 'NN'), ('a', 'DT'), ('standard', 'JJ'), ('powerbar', 'NN'), ('has', 'VBZ'), ('16to17', 'CD'), ('inches', 'NNS'), ('between', 'IN'), ('the', 'DT'), ('ends', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('inside', 'JJ'), ('neural', 'JJ'), ('and', 'CC'), ('32', 'CD'), ('inches', 'NNS'), ('between', 'IN'), ('the', 'DT'), ('finger', 'NN'), ('marks', 'NNS')]\n",
      "(S\n",
      "  again/RB\n",
      "  heels/NNS\n",
      "  should/MD\n",
      "  be/VB\n",
      "  about/IN\n",
      "  shoulder/NN\n",
      "  width/NNS\n",
      "  apart/RB\n",
      "  with/IN\n",
      "  toes/NNS\n",
      "  pointed/VBN\n",
      "  out/RP\n",
      "  about/IN\n",
      "  30/CD\n",
      "  degrees/NNS\n",
      "  at/IN\n",
      "  this/DT\n",
      "  point/NN\n",
      "  you/PRP\n",
      "  are/VBP\n",
      "  ready/JJ\n",
      "  to/TO\n",
      "  squat/VB\n",
      "  with/IN\n",
      "  the/DT\n",
      "  empty/JJ\n",
      "  bar/NN)\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('treebank')\n",
    "# print(sents[30])\n",
    "sent = nltk.corpus.treebank.tagged_sents()[22]\n",
    "#print(sent)\n",
    "print(sents[15])\n",
    "#print(nltk.ne_chunk(sent))\n",
    "print(nltk.ne_chunk(sents[25]))\n",
    "\n",
    "# sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\n",
    "# (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "\n",
    "# grammar = \"NP: {<DT>?<JJ>*<NN>}\" \n",
    "\n",
    "# cp = nltk.RegexpParser(grammar) \n",
    "# result = cp.parse(sentence) \n",
    "# print(result) \n",
    "grammar = r\"NP: {<[CDJNP].*>+}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "# print(cp.evaluate(sents))\n",
    "# result.draw() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding sentences with technique rules(unfinished) (regexp: noun(body part) and verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Damian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we will use a fairly neutral foot placement with the heels about shoulder width apart and the toes pointed out at about 30 degrees many people will assume a stance with toes pointed too forward so you may need to point them out more than you want\n",
      "your back should be as flat as you can get it\n",
      "also notice that your back is inclined at about a45 degree angle not at all vertical and your eyes are looking down at the floor a few feet in front of you\n",
      "this movement keeps your weight solidly over the whole foot instead of letting it shift to the toes\n",
      "grip width for the squat will vary with shoulder width and flexibility but in general the hands will be between these two markings\n",
      "the elbows should be lifted up to trap the bar between the hands and the back elbows should be up but not high\n",
      "with your grip in place and your hands and thumbs on top of the bar dip your head under the bar and come up into position with the bar on your back just below the spine of the scapula the bone you feel at the top of the shoulder blades and then secure it in place by lifting your elbows and chest at the same time\n",
      "take the bar out of the rack in the same position in which it is to be squatted with the torso and shoulders tight the chest and elbows up the head position down and both feet under the bar\n",
      "step back just enough to clear the rack and assume the same stance you used earlier\n",
      "again heels should be about shoulder width apart with toes pointed out about 30 degrees at this point you are ready to squat with the empty bar\n",
      "and two dont stop at the bottom just go down and immediately come back up driving your butt straight up not forward not back out of the bottom\n",
      "you should be in good balance at the bottom of the squat with your weight balanced evenly over your feet neither on your heels nor forward on your toes balance problems usually indicate a back angle that is too vertical remember that the back angle will not be vertical at all sit back lean forward shove your knees out point your nipples at the floor allow your hips to perform the squat not your legs\n"
     ]
    }
   ],
   "source": [
    "text = \"When performing squats with a barbell, ensure your back is straight, knees do not extend beyond your toes, and the barbell rests securely on your shoulders.\"\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk \n",
    "nltk.download('wordnet')\n",
    "part = wn.synsets('body_part')[0]\n",
    "\n",
    "def is_body_part(candidate):\n",
    "    for ss in wn.synsets(candidate):\n",
    "        # only get those where the synset matches exactly\n",
    "        name = ss.name().split(\".\", 1)[0]\n",
    "        if name != candidate:\n",
    "            continue\n",
    "        hit = part.lowest_common_hypernyms(ss)\n",
    "        if hit and hit[0] == part:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# for word in sents[0]:\n",
    "#     print(is_body_part(word[0]), word[0], sep=\"\\t\")\n",
    "\n",
    "# Procesowanie każdego zdania\n",
    "# for sentence in sents:\n",
    "#     if any(is_body_part(t[0].lower()) for t in sentence):\n",
    "#         print(f\"Zdanie zawiera część ciała: {sentence}\")\n",
    "\n",
    "import nltk\n",
    "from nltk import CFG\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.parse import ChartParser\n",
    "\n",
    "# Lista części ciała\n",
    "body_parts = [\"head\", \"arm\", \"leg\", \"hand\", \"foot\", \"eye\", \"ear\", \"nose\", \"mouth\", \"shoulder\", \"knee\", \"elbow\"]\n",
    "\n",
    "# Definicja gramatyki bezkontekstowej z użyciem POS tags\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  NP -> DT JJNN | JJNN\n",
    "  VP -> VBZ NP | VBZ ADJP | VBZ PP\n",
    "  DT -> 'the' | 'a' | 'his' | 'her'\n",
    "  JJNN -> JJ NN | JJNN JJ NN\n",
    "  JJ -> 'badly' | 'quickly' | 'slowly' | 'fast'\n",
    "  NN -> 'head' | 'arm' | 'leg' | 'hand' | 'foot' | 'eye' | 'ear' | 'nose' | 'mouth' | 'shoulder' | 'knee' | 'elbow'\n",
    "  VBZ -> VB\n",
    "  ADJP -> JJ NP\n",
    "  NP -> DT JJNN\n",
    "  PP -> IN NP\n",
    "  IN -> 'in'\n",
    "\"\"\")\n",
    "\n",
    "# Tworzenie parsera\n",
    "parser = ChartParser(grammar)\n",
    "\n",
    "# Tokenizacja tekstu na zdania\n",
    "for sentence in sents:\n",
    "    # Sprawdzanie czy zdanie pasuje do gramatyki\n",
    "    #print(sentence)\n",
    "    words = [word for word, tag in sentence]\n",
    "    try:\n",
    "        for tree in parser.parse(words):\n",
    "            # # Sprawdzanie czy pierwsza fraza rzeczownikowa jest częścią ciała\n",
    "            # np = tree[0]\n",
    "            # if np.label() == 'NP' and np[0][0].lower() in body_parts:\n",
    "            #     print(f\"Zdanie zawiera część ciała jako podmiot: {sentence}\")\n",
    "            tree.pretty_print()\n",
    "    except ValueError:\n",
    "            # Jeżeli parser nie znajdzie pasującego drzewa, przechodzi do następnego zdania\n",
    "        #print(\"nie ma drzewa\")    \n",
    "        continue\n",
    "import re \n",
    "\n",
    "#pattern = r'(head|arm|leg|hand|foot|eye|ear|nose|back|mouth|shoulder|knee|elbow)'\n",
    "pattern = r'\\b(head|arm|leg|hand|foot|eye|ear|nose|back|mouth|shoulder|knee|elbow)\\b\\s+(\\w+)'\n",
    "\n",
    "sents_for_regexp = nltk.sent_tokenize(result)\n",
    "sents_for_regexp = [(segment(sent)) for sent in sents_for_regexp]\n",
    "for sentence in sents_for_regexp:\n",
    "    #print(sentence)\n",
    "    sem = ' '.join([str(elem) for elem in sentence])\n",
    "    #print(sem)\n",
    "    match = re.search(pattern,sem)\n",
    "    if match:\n",
    "        print(sem)\n",
    "        #print(match.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we will approach the squat in two phases first unloaded to solve problems associated with the bottom position and then loaded to learn how to apply the bottom position to the hip drive used for heavier weights \n",
      "\n",
      "since the majority of the problems with the squat happened at the bottom this method expedites the process quite effectively \n",
      "\n",
      "we will use a fairly neutral foot placement with the heels about shoulder width apart and the toes pointed out at about 30 degrees many people will assume a stance with toes pointed too forward so you may need to point them out more than you want \n",
      "\n",
      "to next your e going to assume the position you will be in at the bottom of a squat without the bar squat down all the way to a position in which the apex of the hip crease drops just below the top of the patella \n",
      "\n",
      "put your elbows against your knees with the palms of your hands together and shove your knees out \n",
      "\n",
      "notice your feet are flat on the floor \n",
      "\n",
      "your knees are shoved out to where they are in a parallel line with your feet and just a little in front of your toes \n",
      "\n",
      "your back should be as flat as you can get it \n",
      "\n",
      "also notice that your back is inclined at about a45 degree angle not at all vertical and your eyes are looking down at the floor a few feet in front of you \n",
      "\n",
      "after youve established the bottom position come up out of the bottom by driving your butt straight up in the air \n",
      "\n",
      "up not forward not back \n",
      "\n",
      "this movement keeps your weight solidly over the whole foot instead of letting it shift to the toes \n",
      "\n",
      "think about a chain hook to your hips pulling you straight up out of the bottom \n",
      "\n",
      "set the rack height so that the bar is at about the level of your mid sternum \n",
      "\n",
      "take an even grip on the bar \n",
      "\n",
      "measured from the markings placed on the bar for this purpose a standard powerbar has 16to17 inches between the ends of the inside neural and 32 inches between the finger marks \n",
      "\n",
      "grip width for the squat will vary with shoulder width and flexibility but in general the hands will be between these two markings \n",
      "\n",
      "with the narrowest grip you can manage \n",
      "\n",
      "a narrower grip allows a flexible person to better support the bar with the posterior muscles of the shoulders and a wider grip allows an inflexible person to get more comfortable under the bar \n",
      "\n",
      "the thumbs should be placed on top of the bar so that the wrists can be held in a straight line with the forearms \n",
      "\n",
      "the elbows should be lifted up to trap the bar between the hands and the back elbows should be up but not high \n",
      "\n",
      "with your grip in place and your hands and thumbs on top of the bar dip your head under the bar and come up into position with the bar on your back just below the spine of the scapula the bone you feel at the top of the shoulder blades and then secure it in place by lifting your elbows and chest at the same time \n",
      "\n",
      "it should feel as though the bar is resting on a shelf under the traps and on top of the posterior deltoids \n",
      "\n",
      "take the bar out of the rack in the same position in which it is to be squatted with the torso and shoulders tight the chest and elbows up the head position down and both feet under the bar \n",
      "\n",
      "step back just enough to clear the rack and assume the same stance you used earlier \n",
      "\n",
      "again heels should be about shoulder width apart with toes pointed out about 30 degrees at this point you are ready to squat with the empty bar \n",
      "\n",
      "everything you re about to do is the same as you did unweighted \n",
      "\n",
      "only two things are different one you dont have your elbows available to help push your knees out so you need to do this with your brain \n",
      "\n",
      "and two dont stop at the bottom just go down and immediately come back up driving your butt straight up not forward not back out of the bottom \n",
      "\n",
      "now look down at a spot on the floor about four to five feet in front of you take a big breath and hold it and squat \n",
      "\n",
      "you should be in good balance at the bottom of the squat with your weight balanced evenly over your feet neither on your heels nor forward on your toes balance problems usually indicate a back angle that is too vertical remember that the back angle will not be vertical at all sit back lean forward shove your knees out point your nipples at the floor allow your hips to perform the squat not your legs \n",
      "\n",
      "do not accept anything less than full depth ever \n",
      "\n",
      "if you are high it is usually because your knees are not out \n",
      "\n",
      "most people who have problems with the squat do not shove their knees out enough \n",
      "\n",
      "do a set of five and rack the bar walk forward until the bar touches the vertical parts of the rack \n",
      "\n",
      "find the uprights not the hooks \n",
      "\n",
      "you cant miss the uprights and if you touch them youll be over the hooks \n",
      "\n",
      "the general plan is to do a couple more sets of 5reps with the empty bar to nail down the movement pattern and then add weight do another set of 5 and keep increasing in even increments until the next increase would compromise your form \n",
      "\n",
      "and that is the first squat workout music \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in sents:\n",
    "    for tuple in sent:\n",
    "        print(tuple[0],end = \" \")\n",
    "    print(\"\\n\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main loop of the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'landmark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-a7b3005332d1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mdetection_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mbody_angle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_body_rotation_angle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetection_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpose_landmarks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody_angle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFONT_HERSHEY_SIMPLEX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLINE_AA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-6f60d52f5485>\u001b[0m in \u001b[0;36mcalculate_body_rotation_angle\u001b[1;34m(landmarks)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalculate_body_rotation_angle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlandmarks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# Wykryte punkty charakterystyczne dla ramion (np. 11 i 12 dla lewego i prawego ramienia)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     left_shoulder = [landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER].x,\n\u001b[0m\u001b[0;32m     30\u001b[0m                      landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER].y]\n\u001b[0;32m     31\u001b[0m     right_shoulder = [landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER].x,\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'landmark'"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(video_file)\n",
    "dq = collections.deque()\n",
    "cv2.namedWindow('Video with Subtitles', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('Video with Subtitles', 800, 600)\n",
    "\n",
    "current_frame = 0 \n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "curr_sub_start = 0\n",
    "while True:\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
    "    current_time = current_frame / fps\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "\n",
    "    while(subtitles[curr_sub_start]['start']<current_time):\n",
    "        print(subtitles[curr_sub_start]['text'])\n",
    "        dq.append(curr_sub_start)\n",
    "        curr_sub_start=curr_sub_start+1\n",
    "    if(len(dq) >0):\n",
    "        while(subtitles[dq[0]]['start'] + subtitles[dq[0]]['duration']<current_time):\n",
    "            dq.popleft()\n",
    "    \n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    edges = cv2.Canny(gray_frame, threshold1=100, threshold2=200)  \n",
    "\n",
    "    sub_index=0\n",
    "    for x in dq:\n",
    "        cv2.putText(frame, subtitles[x]['text'], (50, 50+50*sub_index), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        sub_index+=1\n",
    "\n",
    "\n",
    "    img = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "\n",
    "    detection_result = detector.detect(img)\n",
    "    # body_angle = calculate_body_rotation_angle(detection_result.pose_landmarks.landmark)\n",
    "    # cv2.putText(frame, body_angle, (50, 500), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "    annotated_image = draw_landmarks_on_image(img.numpy_view(), detection_result)\n",
    "    bgr_image = cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Display the image using OpenCV\n",
    "    #cv2.imshow('Video with Subtitles', frame)\n",
    "    cv2.imshow('Video with Subtitles', bgr_image)\n",
    "    #cv2.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))    # Wait for user input (right arrow key to go to the next frame)\n",
    "    key = cv2.waitKey(30)  # Adjust the delay as needed (milliseconds)\n",
    "    if key == 27:  # ESC key to exit\n",
    "        break\n",
    "    elif key == 83 or key == 100:\n",
    "        current_frame += 1\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
